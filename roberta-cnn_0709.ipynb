{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #LB 0.709"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parul Pandey님의 https://www.kaggle.com/parulpandey/eda-and-preprocessing-for-bert    \n",
    "https://www.kaggle.com/raenish/tweet-sentiment-insight-eda 에서 영감을 받았습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 파일시스템 관련\n",
    "import os\n",
    "\n",
    "# 데이터 처리 관련\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 텍스트 전처리 관련\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 데이터시각화 관련\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from plotly.offline import iplot\n",
    "import cufflinks\n",
    "cufflinks.go_offline()\n",
    "cufflinks.set_config_file(world_readable=True, theme='pearl')\n",
    "\n",
    "# sklearn \n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Transformers\n",
    "# 4.4.0에서 버전문제 발생하여 3.3.1로 reinstall\n",
    "from transformers import BertTokenizer\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (21984, 4)\n",
      "Testing data shape: (5496, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b7797a8477</td>\n",
       "      <td>I created my account just to get a chance to ...</td>\n",
       "      <td>: I ADMIRE YOU! you`re amazing! you inspire me...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cbdb23868f</td>\n",
       "      <td>too kind Jim too kind brother</td>\n",
       "      <td>kind</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69879e6134</td>\n",
       "      <td>Oh, I`ve just watched the third episode of JON...</td>\n",
       "      <td>it`s awesome.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>169ca458b0</td>\n",
       "      <td>GrimeStopper loss  client lost their job so we...</td>\n",
       "      <td>GrimeStopper loss  client lost their job so we...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0dbaca8e07</td>\n",
       "      <td>goood</td>\n",
       "      <td>goood</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  b7797a8477   I created my account just to get a chance to ...   \n",
       "1  cbdb23868f                      too kind Jim too kind brother   \n",
       "2  69879e6134  Oh, I`ve just watched the third episode of JON...   \n",
       "3  169ca458b0  GrimeStopper loss  client lost their job so we...   \n",
       "4  0dbaca8e07                                              goood   \n",
       "\n",
       "                                       selected_text sentiment  \n",
       "0  : I ADMIRE YOU! you`re amazing! you inspire me...  positive  \n",
       "1                                               kind  positive  \n",
       "2                                      it`s awesome.  positive  \n",
       "3  GrimeStopper loss  client lost their job so we...   neutral  \n",
       "4                                              goood  positive  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training/test data\n",
    "train_ori = pd.read_csv(\"./train_split.csv\")\n",
    "test_ori = pd.read_csv(\"./test_split.csv\")\n",
    "print(\"Training data shape:\", train_ori.shape)\n",
    "print(\"Testing data shape:\", test_ori.shape)\n",
    "\n",
    "# train셋에서 첫 5 행 출력\n",
    "train_ori.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자카드 스코어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA 후 train을 다시 train/test 로 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textID           0\n",
       "text             0\n",
       "selected_text    0\n",
       "sentiment        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ori.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null process\n",
    "train_ori.dropna(axis=0, how=\"any\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textID           0\n",
       "text             0\n",
       "selected_text    0\n",
       "sentiment        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ori.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21984, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ori.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral     8919\n",
      "positive    6838\n",
      "negative    6227\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "neutral     0.405704\n",
      "positive    0.311044\n",
      "negative    0.283251\n",
      "Name: sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train_ori.sentiment.value_counts())\n",
    "print()\n",
    "print(train_ori.sentiment.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train을 train, test로 나눈다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# train, test = train_test_split(train_ori,\n",
    "#                                 test_size = 0.2,\n",
    "#                                 random_state = 123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train.shape)\n",
    "# print(test.shape)\n",
    "# print(train.shape[0] + _test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv(\"./train_split_v1.csv\")\n",
    "# test.to_csv(\"./test_split_v1.csv\")\n",
    "\n",
    "# train_t = pd.read_csv(\"./train_split_v1.csv\")\n",
    "# train_t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyuyoo code merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./train_split.csv\")\n",
    "test = pd.read_csv(\"./test_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85a02242e3</td>\n",
       "      <td>Its been a slow day at home, one of my kids is...</td>\n",
       "      <td>Its been a slow day at home, one of my kids is...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a0e0ed4311</td>\n",
       "      <td>about to head to Starbucks. was gonna take the...</td>\n",
       "      <td>missed</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f5f2a709ca</td>\n",
       "      <td>I like them. I mean I like what`s left of them.</td>\n",
       "      <td>I like them.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>064985c587</td>\n",
       "      <td>Grad present photos on facebook</td>\n",
       "      <td>Grad present photos on facebook</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5ab37ce023</td>\n",
       "      <td>That`s kinda cute, to be honest</td>\n",
       "      <td>kinda cute, to be honest</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  85a02242e3  Its been a slow day at home, one of my kids is...   \n",
       "1  a0e0ed4311  about to head to Starbucks. was gonna take the...   \n",
       "2  f5f2a709ca    I like them. I mean I like what`s left of them.   \n",
       "3  064985c587                    Grad present photos on facebook   \n",
       "4  5ab37ce023                    That`s kinda cute, to be honest   \n",
       "\n",
       "                                       selected_text sentiment  \n",
       "0  Its been a slow day at home, one of my kids is...   neutral  \n",
       "1                                             missed  negative  \n",
       "2                                       I like them.  positive  \n",
       "3                    Grad present photos on facebook   neutral  \n",
       "4                           kinda cute, to be honest  positive  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral     8919\n",
      "positive    6838\n",
      "negative    6227\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "neutral     0.405704\n",
      "positive    0.311044\n",
      "negative    0.283251\n",
      "Name: sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train.sentiment.value_counts())\n",
    "print()\n",
    "print(train.sentiment.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral     2198\n",
      "positive    1744\n",
      "negative    1554\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "neutral     0.399927\n",
      "positive    0.317322\n",
      "negative    0.282751\n",
      "Name: sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(test.sentiment.value_counts())\n",
    "print()\n",
    "print(test.sentiment.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> train, test의 감정별 트윗 비율이 거의 유사하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test를 X, y로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85a02242e3</td>\n",
       "      <td>Its been a slow day at home, one of my kids is...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a0e0ed4311</td>\n",
       "      <td>about to head to Starbucks. was gonna take the...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f5f2a709ca</td>\n",
       "      <td>I like them. I mean I like what`s left of them.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment\n",
       "0  85a02242e3  Its been a slow day at home, one of my kids is...   neutral\n",
       "1  a0e0ed4311  about to head to Starbucks. was gonna take the...  negative\n",
       "2  f5f2a709ca    I like them. I mean I like what`s left of them.  positive"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = pd.DataFrame(test, columns=[\"textID\",\"text\",\"sentiment\"])\n",
    "test_x.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Its been a slow day at home, one of my kids is...\n",
       "1                                               missed\n",
       "2                                         I like them.\n",
       "3                      Grad present photos on facebook\n",
       "4                             kinda cute, to be honest\n",
       "Name: selected_text, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y = test.selected_text\n",
    "test_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5496, 3) (5496,)\n",
      "Index(['textID', 'text', 'sentiment'], dtype='object') selected_text\n"
     ]
    }
   ],
   "source": [
    "print(test_x.shape, test_y.shape)\n",
    "print(test_x.columns, test_y.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.712 roBERTa model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, split -> merged from hyuyoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train():\n",
    "    train=pd.read_csv('./train_split.csv')\n",
    "    train['text']=train['text'].astype(str)\n",
    "    train['selected_text']=train['selected_text'].astype(str)\n",
    "    return train\n",
    "\n",
    "def read_test():\n",
    "    test=pd.read_csv('./test_split.csv')\n",
    "    test['text']=test['text'].astype(str)\n",
    "    return test\n",
    "\n",
    "# def read_submission():\n",
    "#     test=pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n",
    "#     return test\n",
    "    \n",
    "train_df = read_train()\n",
    "test_df_ori = read_test()\n",
    "# submission_df = read_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b7797a8477</td>\n",
       "      <td>I created my account just to get a chance to ...</td>\n",
       "      <td>: I ADMIRE YOU! you`re amazing! you inspire me...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cbdb23868f</td>\n",
       "      <td>too kind Jim too kind brother</td>\n",
       "      <td>kind</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  b7797a8477   I created my account just to get a chance to ...   \n",
       "1  cbdb23868f                      too kind Jim too kind brother   \n",
       "\n",
       "                                       selected_text sentiment  \n",
       "0  : I ADMIRE YOU! you`re amazing! you inspire me...  positive  \n",
       "1                                               kind  positive  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21984, 4) (5496, 4)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape, test_df_ori.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85a02242e3</td>\n",
       "      <td>Its been a slow day at home, one of my kids is...</td>\n",
       "      <td>Its been a slow day at home, one of my kids is...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a0e0ed4311</td>\n",
       "      <td>about to head to Starbucks. was gonna take the...</td>\n",
       "      <td>missed</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  85a02242e3  Its been a slow day at home, one of my kids is...   \n",
       "1  a0e0ed4311  about to head to Starbucks. was gonna take the...   \n",
       "\n",
       "                                       selected_text sentiment  \n",
       "0  Its been a slow day at home, one of my kids is...   neutral  \n",
       "1                                             missed  negative  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_ori.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(test_df_ori, columns=[\"textID\",\"text\",\"sentiment\"])\n",
    "answer_st = test_df_ori[\"selected_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['textID' 'text' 'sentiment'] , selected_text\n"
     ]
    }
   ],
   "source": [
    "print(test_df.columns.values, \",\", answer_st.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85a02242e3</td>\n",
       "      <td>Its been a slow day at home, one of my kids is...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a0e0ed4311</td>\n",
       "      <td>about to head to Starbucks. was gonna take the...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f5f2a709ca</td>\n",
       "      <td>I like them. I mean I like what`s left of them.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>064985c587</td>\n",
       "      <td>Grad present photos on facebook</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5ab37ce023</td>\n",
       "      <td>That`s kinda cute, to be honest</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment\n",
       "0  85a02242e3  Its been a slow day at home, one of my kids is...   neutral\n",
       "1  a0e0ed4311  about to head to Starbucks. was gonna take the...  negative\n",
       "2  f5f2a709ca    I like them. I mean I like what`s left of them.  positive\n",
       "3  064985c587                    Grad present photos on facebook   neutral\n",
       "4  5ab37ce023                    That`s kinda cute, to be honest  positive"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Its been a slow day at home, one of my kids is...\n",
       "1                                               missed\n",
       "2                                         I like them.\n",
       "3                      Grad present photos on facebook\n",
       "4                             kinda cute, to be honest\n",
       "Name: selected_text, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_st[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 96\n",
    "PATH = './input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = train_df.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(train_df.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask[k,:len(enc.ids)+5] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+1] = 1\n",
    "        end_tokens[k,toks[-1]+1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = test_df.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test_df.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test_df.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    return 3e-5 * 0.2**epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.3)(x[0]) \n",
    "    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.3)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21984, 4) (5496, 3)\n"
     ]
    }
   ],
   "source": [
    "# checkup\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFRobertaModel.\n",
      "\n",
      "All the weights of TFRobertaModel were initialized from the model checkpoint at ./input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0342 - activation_loss: 0.0170 - activation_1_loss: 0.0172\n",
      "Epoch 00001: val_loss improved from inf to 0.02913, saving model to v2-roberta-0.h5\n",
      "2199/2199 [==============================] - 355s 161ms/step - loss: 0.0342 - activation_loss: 0.0170 - activation_1_loss: 0.0172 - val_loss: 0.0291 - val_activation_loss: 0.0144 - val_activation_1_loss: 0.0147 - lr: 3.0000e-05\n",
      "Epoch 2/5\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0259 - activation_loss: 0.0132 - activation_1_loss: 0.0128\n",
      "Epoch 00002: val_loss improved from 0.02913 to 0.02715, saving model to v2-roberta-0.h5\n",
      "2199/2199 [==============================] - 340s 154ms/step - loss: 0.0259 - activation_loss: 0.0132 - activation_1_loss: 0.0128 - val_loss: 0.0272 - val_activation_loss: 0.0138 - val_activation_1_loss: 0.0133 - lr: 6.0000e-06\n",
      "Epoch 3/5\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0239 - activation_loss: 0.0122 - activation_1_loss: 0.0117\n",
      "Epoch 00003: val_loss did not improve from 0.02715\n",
      "2199/2199 [==============================] - 347s 158ms/step - loss: 0.0239 - activation_loss: 0.0122 - activation_1_loss: 0.0117 - val_loss: 0.0274 - val_activation_loss: 0.0139 - val_activation_1_loss: 0.0134 - lr: 1.2000e-06\n",
      "Epoch 4/5\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0233 - activation_loss: 0.0119 - activation_1_loss: 0.0114\n",
      "Epoch 00004: val_loss did not improve from 0.02715\n",
      "2199/2199 [==============================] - 327s 149ms/step - loss: 0.0233 - activation_loss: 0.0119 - activation_1_loss: 0.0114 - val_loss: 0.0275 - val_activation_loss: 0.0139 - val_activation_1_loss: 0.0135 - lr: 2.4000e-07\n",
      "Epoch 5/5\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0232 - activation_loss: 0.0119 - activation_1_loss: 0.0113\n",
      "Epoch 00005: val_loss did not improve from 0.02715\n",
      "2199/2199 [==============================] - 329s 150ms/step - loss: 0.0232 - activation_loss: 0.0119 - activation_1_loss: 0.0113 - val_loss: 0.0275 - val_activation_loss: 0.0139 - val_activation_1_loss: 0.0135 - lr: 4.8000e-08\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "138/138 [==============================] - 22s 159ms/step\n",
      ">>>> FOLD 1 Jaccard = 0.7095669039332976\n",
      "\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFRobertaModel.\n",
      "\n",
      "All the weights of TFRobertaModel were initialized from the model checkpoint at ./input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0337 - activation_loss: 0.0169 - activation_1_loss: 0.0168\n",
      "Epoch 00001: val_loss improved from inf to 0.02807, saving model to v2-roberta-1.h5\n",
      "2199/2199 [==============================] - 323s 147ms/step - loss: 0.0337 - activation_loss: 0.0169 - activation_1_loss: 0.0168 - val_loss: 0.0281 - val_activation_loss: 0.0145 - val_activation_1_loss: 0.0136 - lr: 3.0000e-05\n",
      "Epoch 2/5\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0263 - activation_loss: 0.0134 - activation_1_loss: 0.0130\n",
      "Epoch 00002: val_loss improved from 0.02807 to 0.02680, saving model to v2-roberta-1.h5\n",
      "2199/2199 [==============================] - 323s 147ms/step - loss: 0.0263 - activation_loss: 0.0134 - activation_1_loss: 0.0130 - val_loss: 0.0268 - val_activation_loss: 0.0138 - val_activation_1_loss: 0.0130 - lr: 6.0000e-06\n",
      "Epoch 3/5\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0245 - activation_loss: 0.0125 - activation_1_loss: 0.0120\n",
      "Epoch 00003: val_loss did not improve from 0.02680\n",
      "2199/2199 [==============================] - 330s 150ms/step - loss: 0.0245 - activation_loss: 0.0125 - activation_1_loss: 0.0120 - val_loss: 0.0270 - val_activation_loss: 0.0139 - val_activation_1_loss: 0.0131 - lr: 1.2000e-06\n",
      "Epoch 4/5\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0240 - activation_loss: 0.0122 - activation_1_loss: 0.0118\n",
      "Epoch 00004: val_loss did not improve from 0.02680\n",
      "2199/2199 [==============================] - 341s 155ms/step - loss: 0.0240 - activation_loss: 0.0122 - activation_1_loss: 0.0118 - val_loss: 0.0271 - val_activation_loss: 0.0140 - val_activation_1_loss: 0.0131 - lr: 2.4000e-07\n",
      "Epoch 5/5\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0238 - activation_loss: 0.0121 - activation_1_loss: 0.0117\n",
      "Epoch 00005: val_loss did not improve from 0.02680\n",
      "2199/2199 [==============================] - 332s 151ms/step - loss: 0.0238 - activation_loss: 0.0121 - activation_1_loss: 0.0117 - val_loss: 0.0271 - val_activation_loss: 0.0140 - val_activation_1_loss: 0.0131 - lr: 4.8000e-08\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "138/138 [==============================] - 21s 149ms/step\n",
      ">>>> FOLD 2 Jaccard = 0.7074861347286658\n",
      "\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFRobertaModel.\n",
      "\n",
      "All the weights of TFRobertaModel were initialized from the model checkpoint at ./input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0342 - activation_loss: 0.0171 - activation_1_loss: 0.0170\n",
      "Epoch 00001: val_loss improved from inf to 0.02854, saving model to v2-roberta-2.h5\n",
      "2199/2199 [==============================] - 323s 147ms/step - loss: 0.0342 - activation_loss: 0.0171 - activation_1_loss: 0.0170 - val_loss: 0.0285 - val_activation_loss: 0.0144 - val_activation_1_loss: 0.0142 - lr: 3.0000e-05\n",
      "Epoch 2/5\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0262 - activation_loss: 0.0133 - activation_1_loss: 0.0129\n",
      "Epoch 00002: val_loss improved from 0.02854 to 0.02664, saving model to v2-roberta-2.h5\n",
      "2199/2199 [==============================] - 329s 150ms/step - loss: 0.0262 - activation_loss: 0.0133 - activation_1_loss: 0.0129 - val_loss: 0.0266 - val_activation_loss: 0.0136 - val_activation_1_loss: 0.0131 - lr: 6.0000e-06\n",
      "Epoch 3/5\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0243 - activation_loss: 0.0125 - activation_1_loss: 0.0118\n",
      "Epoch 00003: val_loss did not improve from 0.02664\n",
      "2199/2199 [==============================] - 340s 154ms/step - loss: 0.0243 - activation_loss: 0.0125 - activation_1_loss: 0.0118 - val_loss: 0.0270 - val_activation_loss: 0.0137 - val_activation_1_loss: 0.0133 - lr: 1.2000e-06\n",
      "Epoch 4/5\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0238 - activation_loss: 0.0122 - activation_1_loss: 0.0116\n",
      "Epoch 00004: val_loss did not improve from 0.02664\n",
      "2199/2199 [==============================] - 355s 161ms/step - loss: 0.0238 - activation_loss: 0.0122 - activation_1_loss: 0.0116 - val_loss: 0.0270 - val_activation_loss: 0.0137 - val_activation_1_loss: 0.0133 - lr: 2.4000e-07\n",
      "Epoch 5/5\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0237 - activation_loss: 0.0121 - activation_1_loss: 0.0115\n",
      "Epoch 00005: val_loss did not improve from 0.02664\n",
      "2199/2199 [==============================] - 349s 159ms/step - loss: 0.0237 - activation_loss: 0.0121 - activation_1_loss: 0.0115 - val_loss: 0.0270 - val_activation_loss: 0.0137 - val_activation_1_loss: 0.0133 - lr: 4.8000e-08\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "138/138 [==============================] - 21s 152ms/step\n",
      ">>>> FOLD 3 Jaccard = 0.7073913992745468\n",
      "\n",
      "#########################\n",
      "### FOLD 4\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFRobertaModel.\n",
      "\n",
      "All the weights of TFRobertaModel were initialized from the model checkpoint at ./input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0333 - activation_loss: 0.0169 - activation_1_loss: 0.0165\n",
      "Epoch 00001: val_loss improved from inf to 0.02878, saving model to v2-roberta-3.h5\n",
      "2199/2199 [==============================] - 332s 151ms/step - loss: 0.0333 - activation_loss: 0.0169 - activation_1_loss: 0.0165 - val_loss: 0.0288 - val_activation_loss: 0.0146 - val_activation_1_loss: 0.0142 - lr: 3.0000e-05\n",
      "Epoch 2/5\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0258 - activation_loss: 0.0132 - activation_1_loss: 0.0126\n",
      "Epoch 00002: val_loss improved from 0.02878 to 0.02769, saving model to v2-roberta-3.h5\n",
      "2199/2199 [==============================] - 352s 160ms/step - loss: 0.0258 - activation_loss: 0.0132 - activation_1_loss: 0.0126 - val_loss: 0.0277 - val_activation_loss: 0.0140 - val_activation_1_loss: 0.0137 - lr: 6.0000e-06\n",
      "Epoch 3/5\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0238 - activation_loss: 0.0122 - activation_1_loss: 0.0116\n",
      "Epoch 00003: val_loss improved from 0.02769 to 0.02747, saving model to v2-roberta-3.h5\n",
      "2199/2199 [==============================] - 357s 162ms/step - loss: 0.0238 - activation_loss: 0.0122 - activation_1_loss: 0.0116 - val_loss: 0.0275 - val_activation_loss: 0.0139 - val_activation_1_loss: 0.0136 - lr: 1.2000e-06\n",
      "Epoch 4/5\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0233 - activation_loss: 0.0120 - activation_1_loss: 0.0114\n",
      "Epoch 00004: val_loss did not improve from 0.02747\n",
      "2199/2199 [==============================] - 346s 157ms/step - loss: 0.0233 - activation_loss: 0.0120 - activation_1_loss: 0.0114 - val_loss: 0.0275 - val_activation_loss: 0.0139 - val_activation_1_loss: 0.0136 - lr: 2.4000e-07\n",
      "Epoch 5/5\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0232 - activation_loss: 0.0119 - activation_1_loss: 0.0113\n",
      "Epoch 00005: val_loss did not improve from 0.02747\n",
      "2199/2199 [==============================] - 327s 148ms/step - loss: 0.0232 - activation_loss: 0.0119 - activation_1_loss: 0.0113 - val_loss: 0.0276 - val_activation_loss: 0.0139 - val_activation_1_loss: 0.0136 - lr: 4.8000e-08\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "138/138 [==============================] - 20s 148ms/step\n",
      ">>>> FOLD 4 Jaccard = 0.706574861921707\n",
      "\n",
      "#########################\n",
      "### FOLD 5\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFRobertaModel.\n",
      "\n",
      "All the weights of TFRobertaModel were initialized from the model checkpoint at ./input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0336 - activation_loss: 0.0170 - activation_1_loss: 0.0166\n",
      "Epoch 00001: val_loss improved from inf to 0.02795, saving model to v2-roberta-4.h5\n",
      "2199/2199 [==============================] - 348s 158ms/step - loss: 0.0336 - activation_loss: 0.0170 - activation_1_loss: 0.0166 - val_loss: 0.0280 - val_activation_loss: 0.0141 - val_activation_1_loss: 0.0138 - lr: 3.0000e-05\n",
      "Epoch 2/5\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0262 - activation_loss: 0.0134 - activation_1_loss: 0.0128\n",
      "Epoch 00002: val_loss improved from 0.02795 to 0.02652, saving model to v2-roberta-4.h5\n",
      "2199/2199 [==============================] - 345s 157ms/step - loss: 0.0262 - activation_loss: 0.0134 - activation_1_loss: 0.0128 - val_loss: 0.0265 - val_activation_loss: 0.0136 - val_activation_1_loss: 0.0130 - lr: 6.0000e-06\n",
      "Epoch 3/5\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0244 - activation_loss: 0.0125 - activation_1_loss: 0.0119\n",
      "Epoch 00003: val_loss did not improve from 0.02652\n",
      "2199/2199 [==============================] - 338s 154ms/step - loss: 0.0244 - activation_loss: 0.0125 - activation_1_loss: 0.0119 - val_loss: 0.0266 - val_activation_loss: 0.0136 - val_activation_1_loss: 0.0130 - lr: 1.2000e-06\n",
      "Epoch 4/5\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0239 - activation_loss: 0.0122 - activation_1_loss: 0.0117\n",
      "Epoch 00004: val_loss did not improve from 0.02652\n",
      "2199/2199 [==============================] - 324s 147ms/step - loss: 0.0239 - activation_loss: 0.0122 - activation_1_loss: 0.0117 - val_loss: 0.0266 - val_activation_loss: 0.0136 - val_activation_1_loss: 0.0130 - lr: 2.4000e-07\n",
      "Epoch 5/5\n",
      "2199/2199 [==============================] - ETA: 0s - loss: 0.0239 - activation_loss: 0.0122 - activation_1_loss: 0.0116\n",
      "Epoch 00005: val_loss did not improve from 0.02652\n",
      "2199/2199 [==============================] - 327s 149ms/step - loss: 0.0239 - activation_loss: 0.0122 - activation_1_loss: 0.0116 - val_loss: 0.0266 - val_activation_loss: 0.0136 - val_activation_1_loss: 0.0130 - lr: 4.8000e-08\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "138/138 [==============================] - 22s 159ms/step\n",
      ">>>> FOLD 5 Jaccard = 0.7031123169776646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jac = []; VER='v2'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=777)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_df.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "        \n",
    "    reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "        \n",
    "    hist = model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "        epochs=5, batch_size=8, verbose=DISPLAY, callbacks=[sv, reduce_lr],\n",
    "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "    \n",
    "    print('Loading model...')\n",
    "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    \n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train_df.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-1:b])\n",
    "        all.append(jaccard(st,train_df.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### MODEL 1\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFRobertaModel.\n",
      "\n",
      "All the weights of TFRobertaModel were initialized from the model checkpoint at ./input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Test...\n",
      "172/172 [==============================] - 27s 154ms/step\n",
      "#########################\n",
      "### MODEL 2\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFRobertaModel.\n",
      "\n",
      "All the weights of TFRobertaModel were initialized from the model checkpoint at ./input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Test...\n",
      "172/172 [==============================] - 25s 148ms/step\n"
     ]
    }
   ],
   "source": [
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "DISPLAY=1\n",
    "for i in range(2):\n",
    "    print('#'*25)\n",
    "    print('### MODEL %i'%(i+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "    model.load_weights('./v2-roberta-%i.h5'%i)\n",
    "\n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/n_splits\n",
    "    preds_end += preds[1]/n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test_df.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-1:b])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['selected_text'] = all\n",
    "test_df[['textID','selected_text']].to_csv('submission2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5496, 2)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inf = pd.read_csv(\"./submission2.csv\")\n",
    "test_inf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85a02242e3</td>\n",
       "      <td>its been a slow day at home, one of my kids i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a0e0ed4311</td>\n",
       "      <td>missed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                      selected_text\n",
       "0  85a02242e3   its been a slow day at home, one of my kids i...\n",
       "1  a0e0ed4311                                             missed"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Its been a slow day at home, one of my kids is...\n",
       "1                                                  missed\n",
       "2                                            I like them.\n",
       "3                         Grad present photos on facebook\n",
       "4                                kinda cute, to be honest\n",
       "                              ...                        \n",
       "5491    Aw! Tear! I feel special to da family.  Haha t...\n",
       "5492                                        you are right\n",
       "5493    self-portrait week http://unbecominglily.blogs...\n",
       "5494                      The time is not my friend today\n",
       "5495    Finally got a call for marriage counseling 3 d...\n",
       "Name: selected_text, Length: 5496, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average jaccard score: 0.7092020870398547\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for i in range(test_inf.shape[0]):\n",
    "    scores.append(jaccard(test_inf[\"selected_text\"][i], answer_st[i]))\n",
    "print(\"average jaccard score:\", sum(scores) / len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweet-gpu",
   "language": "python",
   "name": "tweetgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
