{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LB 0.703"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.2.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b7797a8477</td>\n",
       "      <td>I created my account just to get a chance to ...</td>\n",
       "      <td>: I ADMIRE YOU! you`re amazing! you inspire me...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cbdb23868f</td>\n",
       "      <td>too kind Jim too kind brother</td>\n",
       "      <td>kind</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69879e6134</td>\n",
       "      <td>Oh, I`ve just watched the third episode of JON...</td>\n",
       "      <td>it`s awesome.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>169ca458b0</td>\n",
       "      <td>GrimeStopper loss  client lost their job so we...</td>\n",
       "      <td>GrimeStopper loss  client lost their job so we...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0dbaca8e07</td>\n",
       "      <td>goood</td>\n",
       "      <td>goood</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  b7797a8477   I created my account just to get a chance to ...   \n",
       "1  cbdb23868f                      too kind Jim too kind brother   \n",
       "2  69879e6134  Oh, I`ve just watched the third episode of JON...   \n",
       "3  169ca458b0  GrimeStopper loss  client lost their job so we...   \n",
       "4  0dbaca8e07                                              goood   \n",
       "\n",
       "                                       selected_text sentiment  \n",
       "0  : I ADMIRE YOU! you`re amazing! you inspire me...  positive  \n",
       "1                                               kind  positive  \n",
       "2                                      it`s awesome.  positive  \n",
       "3  GrimeStopper loss  client lost their job so we...   neutral  \n",
       "4                                              goood  positive  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 96\n",
    "PATH = './input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "train = pd.read_csv('train_split.csv').fillna('')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 학습\n",
    "https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705 의 자료를 사용하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = train.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(train.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask[k,:len(enc.ids)+5] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+1] = 1\n",
    "        end_tokens[k,toks[-1]+1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test 데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_split.csv').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85a02242e3</td>\n",
       "      <td>Its been a slow day at home, one of my kids is...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a0e0ed4311</td>\n",
       "      <td>about to head to Starbucks. was gonna take the...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f5f2a709ca</td>\n",
       "      <td>I like them. I mean I like what`s left of them.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>064985c587</td>\n",
       "      <td>Grad present photos on facebook</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5ab37ce023</td>\n",
       "      <td>That`s kinda cute, to be honest</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5491</th>\n",
       "      <td>9e951e5565</td>\n",
       "      <td>Aw! Tear! I feel special to da family.  Haha t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5492</th>\n",
       "      <td>ebc1e3bcb6</td>\n",
       "      <td>Yes PM PM you are right</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5493</th>\n",
       "      <td>eefc505b2b</td>\n",
       "      <td>self-portrait week http://unbecominglily.blogs...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5494</th>\n",
       "      <td>7f392f9c06</td>\n",
       "      <td>The time is not my friend today</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5495</th>\n",
       "      <td>6ed347b1a5</td>\n",
       "      <td>Finally got a call for marriage counseling 3 d...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5496 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                               text sentiment\n",
       "0     85a02242e3  Its been a slow day at home, one of my kids is...   neutral\n",
       "1     a0e0ed4311  about to head to Starbucks. was gonna take the...  negative\n",
       "2     f5f2a709ca    I like them. I mean I like what`s left of them.  positive\n",
       "3     064985c587                    Grad present photos on facebook   neutral\n",
       "4     5ab37ce023                    That`s kinda cute, to be honest  positive\n",
       "...          ...                                                ...       ...\n",
       "5491  9e951e5565  Aw! Tear! I feel special to da family.  Haha t...  positive\n",
       "5492  ebc1e3bcb6                            Yes PM PM you are right  positive\n",
       "5493  eefc505b2b  self-portrait week http://unbecominglily.blogs...   neutral\n",
       "5494  7f392f9c06                    The time is not my friend today  negative\n",
       "5495  6ed347b1a5  Finally got a call for marriage counseling 3 d...   neutral\n",
       "\n",
       "[5496 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_st = test['selected_text']\n",
    "del test['selected_text']\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Its been a slow day at home, one of my kids is...\n",
       "1                                                  missed\n",
       "2                                            I like them.\n",
       "3                         Grad present photos on facebook\n",
       "4                                kinda cute, to be honest\n",
       "                              ...                        \n",
       "5491    Aw! Tear! I feel special to da family.  Haha t...\n",
       "5492                                        you are right\n",
       "5493    self-portrait week http://unbecominglily.blogs...\n",
       "5494                      The time is not my friend today\n",
       "5495    Finally got a call for marriage counseling 3 d...\n",
       "Name: selected_text, Length: 5496, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFRobertaModel.\n",
      "\n",
      "All the weights of TFRobertaModel were initialized from the model checkpoint at ./input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 96)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 96)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 96)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 96, 768), (N 124645632   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 96, 768)      0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 96, 768)      0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 96, 1)        769         dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 96, 1)        769         dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 96)           0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 96)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 96)           0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 96)           0           flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 124,647,170\n",
      "Trainable params: 124,647,170\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자카드 스코어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFRobertaModel.\n",
      "\n",
      "All the weights of TFRobertaModel were initialized from the model checkpoint at ./input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "458/458 [==============================] - ETA: 0s - loss: 2.3037 - activation_loss: 1.1432 - activation_1_loss: 1.1605\n",
      "Epoch 00001: val_loss improved from inf to 1.69476, saving model to v0-roberta-0.h5\n",
      "458/458 [==============================] - 2307s 5s/step - loss: 2.3037 - activation_loss: 1.1432 - activation_1_loss: 1.1605 - val_loss: 1.6948 - val_activation_loss: 0.8808 - val_activation_1_loss: 0.8139\n",
      "Epoch 2/3\n",
      "458/458 [==============================] - ETA: 0s - loss: 1.6511 - activation_loss: 0.8478 - activation_1_loss: 0.8033\n",
      "Epoch 00002: val_loss improved from 1.69476 to 1.64124, saving model to v0-roberta-0.h5\n",
      "458/458 [==============================] - 2383s 5s/step - loss: 1.6511 - activation_loss: 0.8478 - activation_1_loss: 0.8033 - val_loss: 1.6412 - val_activation_loss: 0.8546 - val_activation_1_loss: 0.7867\n",
      "Epoch 3/3\n",
      "458/458 [==============================] - ETA: 0s - loss: 1.4960 - activation_loss: 0.7681 - activation_1_loss: 0.7280\n",
      "Epoch 00003: val_loss improved from 1.64124 to 1.63105, saving model to v0-roberta-0.h5\n",
      "458/458 [==============================] - 2498s 5s/step - loss: 1.4960 - activation_loss: 0.7681 - activation_1_loss: 0.7280 - val_loss: 1.6311 - val_activation_loss: 0.8440 - val_activation_1_loss: 0.7871\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "229/229 [==============================] - 338s 1s/step\n",
      "Predicting Test...\n",
      "172/172 [==============================] - 244s 1s/step\n",
      ">>>> FOLD 1 Jaccard = 0.708979100753134\n",
      "\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFRobertaModel.\n",
      "\n",
      "All the weights of TFRobertaModel were initialized from the model checkpoint at ./input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "458/458 [==============================] - ETA: 0s - loss: 2.3982 - activation_loss: 1.1663 - activation_1_loss: 1.2318\n",
      "Epoch 00001: val_loss improved from inf to 1.80993, saving model to v0-roberta-1.h5\n",
      "458/458 [==============================] - 2524s 6s/step - loss: 2.3982 - activation_loss: 1.1663 - activation_1_loss: 1.2318 - val_loss: 1.8099 - val_activation_loss: 0.9032 - val_activation_1_loss: 0.9068\n",
      "Epoch 2/3\n",
      "458/458 [==============================] - ETA: 0s - loss: 1.7593 - activation_loss: 0.8943 - activation_1_loss: 0.8650\n",
      "Epoch 00002: val_loss improved from 1.80993 to 1.68999, saving model to v0-roberta-1.h5\n",
      "458/458 [==============================] - 2618s 6s/step - loss: 1.7593 - activation_loss: 0.8943 - activation_1_loss: 0.8650 - val_loss: 1.6900 - val_activation_loss: 0.8666 - val_activation_1_loss: 0.8234\n",
      "Epoch 3/3\n",
      "458/458 [==============================] - ETA: 0s - loss: 1.5693 - activation_loss: 0.8121 - activation_1_loss: 0.7571\n",
      "Epoch 00003: val_loss did not improve from 1.68999\n",
      "458/458 [==============================] - 2452s 5s/step - loss: 1.5693 - activation_loss: 0.8121 - activation_1_loss: 0.7571 - val_loss: 1.7134 - val_activation_loss: 0.8799 - val_activation_1_loss: 0.8334\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "229/229 [==============================] - 346s 2s/step\n",
      "Predicting Test...\n",
      "172/172 [==============================] - 257s 1s/step\n",
      ">>>> FOLD 2 Jaccard = 0.696239611372558\n",
      "\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFRobertaModel.\n",
      "\n",
      "All the weights of TFRobertaModel were initialized from the model checkpoint at ./input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "458/458 [==============================] - ETA: 0s - loss: 2.4046 - activation_loss: 1.1516 - activation_1_loss: 1.2530\n",
      "Epoch 00001: val_loss improved from inf to 1.72688, saving model to v0-roberta-2.h5\n",
      "458/458 [==============================] - 2316s 5s/step - loss: 2.4046 - activation_loss: 1.1516 - activation_1_loss: 1.2530 - val_loss: 1.7269 - val_activation_loss: 0.8671 - val_activation_1_loss: 0.8597\n",
      "Epoch 2/3\n",
      "458/458 [==============================] - ETA: 0s - loss: 1.7130 - activation_loss: 0.8665 - activation_1_loss: 0.8465\n",
      "Epoch 00002: val_loss improved from 1.72688 to 1.66374, saving model to v0-roberta-2.h5\n",
      "458/458 [==============================] - 2297s 5s/step - loss: 1.7130 - activation_loss: 0.8665 - activation_1_loss: 0.8465 - val_loss: 1.6637 - val_activation_loss: 0.8456 - val_activation_1_loss: 0.8181\n",
      "Epoch 3/3\n",
      "458/458 [==============================] - ETA: 0s - loss: 1.5354 - activation_loss: 0.7871 - activation_1_loss: 0.7484\n",
      "Epoch 00003: val_loss did not improve from 1.66374\n",
      "458/458 [==============================] - 2319s 5s/step - loss: 1.5354 - activation_loss: 0.7871 - activation_1_loss: 0.7484 - val_loss: 1.6677 - val_activation_loss: 0.8493 - val_activation_1_loss: 0.8183\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "229/229 [==============================] - 296s 1s/step\n",
      "Predicting Test...\n",
      "172/172 [==============================] - 221s 1s/step\n",
      ">>>> FOLD 3 Jaccard = 0.6921691970050207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3,shuffle=True,random_state=777)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "        \n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "        \n",
    "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
    "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "    \n",
    "    print('Loading model...')\n",
    "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    \n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-1:b])\n",
    "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> OVERALL 5Fold CV Jaccard = 0.6991293030435708\n"
     ]
    }
   ],
   "source": [
    "print('>>>> OVERALL 3Fold CV Jaccard =',np.mean(jac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-1:b])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5151</th>\n",
       "      <td>1635b82ee7</td>\n",
       "      <td>did your car get towed? Thats what happened to redgie`s...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>did your car get towed? thats what happened to redgie`s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4159</th>\n",
       "      <td>71d78dd6c4</td>\n",
       "      <td>Back at home,11 hours till work</td>\n",
       "      <td>neutral</td>\n",
       "      <td>back at home,11 hours till work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2759</th>\n",
       "      <td>dce4480092</td>\n",
       "      <td>And a good morning to you!  Up early with a bad conscie...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>and a good morning to you! up early with a bad conscien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>77d4a716dd</td>\n",
       "      <td>OO YAY 39 FOLLOWERS I WANT 100  HELP ME</td>\n",
       "      <td>positive</td>\n",
       "      <td>help me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>5d0aff483b</td>\n",
       "      <td>@_Mintyfresh Thanks very much!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>thanks very much!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2543</th>\n",
       "      <td>bf73c02eb3</td>\n",
       "      <td>Is listening to the new jonas brothers song</td>\n",
       "      <td>neutral</td>\n",
       "      <td>is listening to the new jonas brothers song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>b068ec2560</td>\n",
       "      <td>Wow - the dude said I was better than Bobbi Lewis.. neve...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wow - the dude said i was better than bobbi lewis.. nev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2713</th>\n",
       "      <td>c70c228685</td>\n",
       "      <td>They just admitted my grandma to the hospital... Shes my...</td>\n",
       "      <td>negative</td>\n",
       "      <td>shes my only last one please pray for her</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5455</th>\n",
       "      <td>c8e658737f</td>\n",
       "      <td>Installed office for Mac &amp; messed up all my fonts on saf...</td>\n",
       "      <td>negative</td>\n",
       "      <td>messed up all my fonts on safari, **** on a stick! help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5017</th>\n",
       "      <td>248497cbac</td>\n",
       "      <td>signed up for broadband today could take 4-6 weeks</td>\n",
       "      <td>neutral</td>\n",
       "      <td>signed up for broadband today could take 4-6 weeks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3425</th>\n",
       "      <td>c2bfbeac47</td>\n",
       "      <td>(OOC: -we`ll do it.  )</td>\n",
       "      <td>neutral</td>\n",
       "      <td>(ooc: -we`ll do it. )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5122</th>\n",
       "      <td>c481433b33</td>\n",
       "      <td>hm... i don`t I can recommend any white chocolates thou...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>hm... i don`t i can recommend any white chocolates thou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>5c6daee4bc</td>\n",
       "      <td>Sounds great. I`m looking forward to it then</td>\n",
       "      <td>positive</td>\n",
       "      <td>sounds great.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4391</th>\n",
       "      <td>a7d82d0125</td>\n",
       "      <td>Come on Knight... It`s been well over 4 hours</td>\n",
       "      <td>neutral</td>\n",
       "      <td>come on knight... it`s been well over 4 hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2421</th>\n",
       "      <td>7f8311af2d</td>\n",
       "      <td>Scrubs tonight (8.00pm). woo!</td>\n",
       "      <td>positive</td>\n",
       "      <td>woo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>b78d02040a</td>\n",
       "      <td>i wish i could meet you once  do u think this will happ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>i wish i could meet you once do u think this will happe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993</th>\n",
       "      <td>30377e00ed</td>\n",
       "      <td>Morning and hope everyone has a great bank holiday</td>\n",
       "      <td>positive</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3836</th>\n",
       "      <td>52a7198645</td>\n",
       "      <td>true, depends on the couple  Personally we`ve found add...</td>\n",
       "      <td>positive</td>\n",
       "      <td>increased the fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3831</th>\n",
       "      <td>ba14fc298b</td>\n",
       "      <td>Awake again , I give up , I`m going to ready for today</td>\n",
       "      <td>neutral</td>\n",
       "      <td>awake again , i give up , i`m going to ready for today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3822</th>\n",
       "      <td>05fa4de496</td>\n",
       "      <td>chale... a ver define 'out'</td>\n",
       "      <td>neutral</td>\n",
       "      <td>chale... a ver define 'out'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>2867546760</td>\n",
       "      <td>if only that was really my job title</td>\n",
       "      <td>neutral</td>\n",
       "      <td>if only that was really my job title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5469</th>\n",
       "      <td>435558d3b8</td>\n",
       "      <td>Now to go home and cry</td>\n",
       "      <td>neutral</td>\n",
       "      <td>now to go home and cry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4515</th>\n",
       "      <td>a662842b79</td>\n",
       "      <td>Enjoying 'Gears of War' on my PC ! This game is really g...</td>\n",
       "      <td>positive</td>\n",
       "      <td>enjoying 'gears of war' on my pc ! this game is really ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>6f64d970e6</td>\n",
       "      <td>woken up by mum. work</td>\n",
       "      <td>neutral</td>\n",
       "      <td>woken up by mum. work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3245</th>\n",
       "      <td>612a1d2579</td>\n",
       "      <td>Oooh! goodluck for the rest of them</td>\n",
       "      <td>positive</td>\n",
       "      <td>goodluck</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                                         text  \\\n",
       "5151  1635b82ee7   did your car get towed? Thats what happened to redgie`s...   \n",
       "4159  71d78dd6c4                              Back at home,11 hours till work   \n",
       "2759  dce4480092   And a good morning to you!  Up early with a bad conscie...   \n",
       "649   77d4a716dd                      OO YAY 39 FOLLOWERS I WANT 100  HELP ME   \n",
       "4175  5d0aff483b                              @_Mintyfresh Thanks very much!!   \n",
       "2543  bf73c02eb3                  Is listening to the new jonas brothers song   \n",
       "542   b068ec2560  Wow - the dude said I was better than Bobbi Lewis.. neve...   \n",
       "2713  c70c228685  They just admitted my grandma to the hospital... Shes my...   \n",
       "5455  c8e658737f  Installed office for Mac & messed up all my fonts on saf...   \n",
       "5017  248497cbac           signed up for broadband today could take 4-6 weeks   \n",
       "3425  c2bfbeac47                                       (OOC: -we`ll do it.  )   \n",
       "5122  c481433b33   hm... i don`t I can recommend any white chocolates thou...   \n",
       "2997  5c6daee4bc                 Sounds great. I`m looking forward to it then   \n",
       "4391  a7d82d0125                Come on Knight... It`s been well over 4 hours   \n",
       "2421  7f8311af2d                                Scrubs tonight (8.00pm). woo!   \n",
       "956   b78d02040a   i wish i could meet you once  do u think this will happ...   \n",
       "3993  30377e00ed           Morning and hope everyone has a great bank holiday   \n",
       "3836  52a7198645   true, depends on the couple  Personally we`ve found add...   \n",
       "3831  ba14fc298b       Awake again , I give up , I`m going to ready for today   \n",
       "3822  05fa4de496                                  chale... a ver define 'out'   \n",
       "2006  2867546760                         if only that was really my job title   \n",
       "5469  435558d3b8                                       Now to go home and cry   \n",
       "4515  a662842b79  Enjoying 'Gears of War' on my PC ! This game is really g...   \n",
       "3049  6f64d970e6                                        woken up by mum. work   \n",
       "3245  612a1d2579                          Oooh! goodluck for the rest of them   \n",
       "\n",
       "     sentiment                                                selected_text  \n",
       "5151   neutral   did your car get towed? thats what happened to redgie`s...  \n",
       "4159   neutral                              back at home,11 hours till work  \n",
       "2759   neutral   and a good morning to you! up early with a bad conscien...  \n",
       "649   positive                                                      help me  \n",
       "4175  positive                                           thanks very much!!  \n",
       "2543   neutral                  is listening to the new jonas brothers song  \n",
       "542   positive   wow - the dude said i was better than bobbi lewis.. nev...  \n",
       "2713  negative                    shes my only last one please pray for her  \n",
       "5455  negative   messed up all my fonts on safari, **** on a stick! help...  \n",
       "5017   neutral           signed up for broadband today could take 4-6 weeks  \n",
       "3425   neutral                                        (ooc: -we`ll do it. )  \n",
       "5122   neutral   hm... i don`t i can recommend any white chocolates thou...  \n",
       "2997  positive                                                sounds great.  \n",
       "4391   neutral                come on knight... it`s been well over 4 hours  \n",
       "2421  positive                                                         woo!  \n",
       "956   negative   i wish i could meet you once do u think this will happe...  \n",
       "3993  positive                                                        great  \n",
       "3836  positive                                            increased the fun  \n",
       "3831   neutral       awake again , i give up , i`m going to ready for today  \n",
       "3822   neutral                                  chale... a ver define 'out'  \n",
       "2006   neutral                         if only that was really my job title  \n",
       "5469   neutral                                       now to go home and cry  \n",
       "4515  positive   enjoying 'gears of war' on my pc ! this game is really ...  \n",
       "3049   neutral                                        woken up by mum. work  \n",
       "3245  positive                                                     goodluck  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['selected_text'] = all\n",
    "test[['textID','selected_text']].to_csv('submission.csv1',index=False)\n",
    "pd.set_option('max_colwidth', 60)\n",
    "test.sample(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스코어 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average jaccard score: 0.7032586629289909\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for i in range(test.shape[0]):\n",
    "    scores.append(jaccard(test[\"selected_text\"][i], ans_st[i]))\n",
    "print(\"average jaccard score:\", sum(scores) / len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 : Its been a slow day at home, one of my kids is sick  .  This little picture cheered me up http://is.gd/JrLa\n",
      "예측 :  its been a slow day at home, one of my kids is sick . this little picture cheered me up\n",
      "정답 : Its been a slow day at home, one of my kids is sick  .  This little picture cheered me up\n"
     ]
    }
   ],
   "source": [
    "print(\"원본 :\",test[\"text\"][0]) \n",
    "print(\"예측 :\",test[\"selected_text\"][0]) \n",
    "print(\"정답 :\",ans_st[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 : Scrubs tonight (8.00pm). woo!\n",
      "예측 :  woo!\n",
      "정답 : woo!\n"
     ]
    }
   ],
   "source": [
    "print(\"원본 :\",test[\"text\"][2421]) \n",
    "print(\"예측 :\",test[\"selected_text\"][2421]) \n",
    "print(\"정답 :\",ans_st[2421])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 : Im sad right now becuz of my lady\n",
      "예측 :  im sad\n",
      "정답 : sad\n"
     ]
    }
   ],
   "source": [
    "print(\"원본 :\",test[\"text\"][87]) \n",
    "print(\"예측 :\",test[\"selected_text\"][87]) \n",
    "print(\"정답 :\",ans_st[87])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "얼추 비슷하게 예측을 한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweet",
   "language": "python",
   "name": "tweet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
